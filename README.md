# AskNEU - MLOps Project [Team - 18]

## Application live : https://tinyurl.com/askneu


AskNEU is a conversational Retrieval-Augmented Generation (RAG) system designed to transform how users interact with Northeastern University's vast repository of information. AskNEU delivers accurate, context-aware answers to user queries in real time by integrating advanced AI language models with targeted data retrieval techniques. Whether you're a prospective student exploring academic programs, a current student navigating campus resources, or a faculty member seeking policy details, AskNEU serves as your intelligent assistant.
 
Traditional information retrieval systems often require users to sift through multiple web pages or documents to find specific answers. AskNEU eliminates this by combining the precision of search engines with the natural language understanding of GPT-based models. When a user asks a question, the system first retrieves the most relevant data from Northeastern's official website and then generates a concise, conversational response tailored to the command. This approach ensures that users receive accurate real-time information.

The project is driven by the goal of providing accurate information for the Northeastern community. AskNEU bridges the gap between complex institutional data and user-friendly communication. Our vision is to create a system that not only answers questions but also anticipates user needs, providing proactive suggestions and insights. This project represents a significant step forward in making institutional knowledge more accessible, intuitive, and engaging for everyone connected to Northeastern University.

---

## Table of Contents
- [Data Pipeline Structure Structure](#data-pipeline-structure)
- [Prerequisites](#prerequisites)
- [Setup Instructions](#setup-instructions)
- [Running the Pipeline](#running-the-pipeline)
- [Code Structure](#code-structure)
- [Reproducibility](#reproducibility)
- [Error Handling & Logging](#error-handling--logging)
- [Expected Outcome](#expected-outcome)
- [Contributing](#contributing)

---

## Data Pipeline Structure

```
/Project Repo
|-- Data-Pipeline/          # Root directory for pipeline-related files
|   |-- dags/               # Airflow DAG files
|   |   |-- scraping_pipeline_gcs.py       # DAG for scraping and GCS upload
|   |   |-- gcs_to_pinecone_embedding.py   # DAG for embedding and Pinecone upsert
|   |-- data/               # Local storage for scraped data
|   |-- scripts/            # Python scripts (e.g., scraping logic)
|   |   |-- Scrape_script.py
|   |   |-- requirements.txt
|   |-- tests/              # Unit tests for scripts
|   |   |-- test_scraper.py
|   |-- logs/               # Log files generated by Airflow
|   |-- dvc.yaml            # DVC configuration for data versioning
|-- docker-compose.yml      # Docker Compose configuration for Airflow
|-- README.md               # Project documentation
```

---

## Prerequisites

- **Docker**: For containerized Airflow setup
- **Docker Compose**: To manage Airflow services
- **Google Cloud SDK**: For GCS access
- **DVC**: For data versioning
- **Pinecone Account**: For embedding storage

---

## Setup Instructions

1. **Clone the Repository**:
   ```bash
   git clone https://github.com/justin-aj/AskNEU.git
   cd AskNEU/Data-Pipeline
   ```

2. **Set Up Docker Compose**:
   - Ensure `docker-compose.yml` is present in the root directory (or create one based on Airflow’s official example).
   - Start Airflow services:
     ```bash
     docker-compose up -d
     ```
   - This launches the Airflow webserver, scheduler, and database.

3. **Access Airflow UI**:
   - Once Airflow is up, open `http://localhost:8080` in your browser.
   - Log in with:
     - **Username**: `airflow`
     - **Password**: `airflow`

4. **Configure GCP Connection**:
   - In the Airflow UI, go to `Admin > Connections`.
   - Add a new connection:
     - **Conn Id**: `gcp_service_account`
     - **Conn Type**: `Google Cloud`
     - **Keyfile JSON**: Paste your GCS service account JSON (e.g., `{"type": "service_account", "project_id": "xxxx", ...}`).
   - Save the connection.

5. **Set Airflow Variables**:
   - Go to `Admin > Variables` in the UI.
   - Add:
     - **Key**: `GCS_BUCKET_NAME`
     - **Value**: Your GCS bucket URL (e.g., `my-gcs-bucket`).
   - Save the variable.

6. **Install DVC** (if not in Docker image):
   - If your Docker image doesn’t include DVC, exec into the container:
     ```bash
     docker exec -it <airflow-container-name> bash
     pip install dvc[gcs]
     ```

7. **Set Up Pinecone**:
   - Update `utils/config_utils.py` with your Pinecone credentials:
     ```python
     "pinecone": {
         "api_key": "your-pinecone-api-key",
         "environment": "your-pinecone-environment",
         "index_name": "your-index-name",
         "namespace": "your-namespace"
     }
     ```

---

## Running the Pipeline

1. **Start Airflow**:
   - If not already running, use:
     ```bash
     docker-compose up -d
     ```
   - Verify services in the Airflow UI at `http://localhost:8080`.

2. **Enable DAGs**:
   - In the UI, toggle on:
     - `scraping_pipeline_gcs`: Scrapes data, validates it, versions with DVC, and uploads to GCS.
     - `gcs_to_pinecone_embedding`: Downloads from GCS, generates embeddings, and upserts to Pinecone.

3. **Monitor Execution**:
   - Check task status in the Airflow UI.
   - Logs are available in the `logs/` directory or via the UI.

---

## Application Architecture

AskNEU follows a three-tier architecture, consisting of:

1. **React Frontend**: User interface that handles the chat experience
2. **Node.js/Express Backend**: API server that processes requests and manages data persistence
3. **Python RAG Service**: Specialized service for retrieval-augmented generation

The system combines MongoDB for data persistence with Pinecone for vector embeddings, creating a comprehensive information retrieval system.

## Backend Structure

The backend (neu-chatbot-backend/) is responsible for handling API requests, storing and retrieving conversation data from MongoDB, and communicating with the LLM service.

```
neu-chatbot-backend/
├── controllers/            # Core logic for chat and feedback processing
├── models/                 # Mongoose models for Conversation and Feedback
├── routes/                 # Express.js API route definitions
├── services/               # Utility services for DB operations and validation
├── python_service.py       # Flask-based Python service for LLM responses
├── index.js                # Main entry point for the Express server
├── .env / secrets.yaml     # Configuration files for MongoDB, ports, and secrets
```

### Key Backend Features

- **User Management**: Google OAuth and guest user support
- **Conversation Persistence**: Stores chat history in MongoDB
- **Feedback Collection**: Captures user ratings on responses
- **API Proxy**: Forwards queries to the Python RAG service

### Running the Backend Locally

Start the Node.js backend and Python service separately:

```bash
# Start the Node.js API server
cd neu-chatbot-backend
npm install
node server.js

# In a new terminal, start the LLM Python service
python python_service.py
```

---

## Frontend Structure

The frontend (neu-chatbot/) is a React-based interface that enables chat interaction, feedback submission, and dynamic UI rendering with markdown support.

```
neu-chatbot/
├── public/                 # Static files (HTML, icons, manifest)
├── src/
│   ├── App.js              # Main React component with chat logic
│   ├── App.css             # Custom styles (including dark mode)
│   ├── LoginScreen.js      # Entry screen for user sessions
│   ├── components/         # (Optional) Modular UI components
│   ├── utils/              # API handlers, constants, helper functions
```
### Key Frontend Features

- **Responsive Chat Interface**: Message history with support for markdown rendering
- **Authentication**: Google login and guest access options
- **Theme Support**: Toggle between light and dark modes
- **Incognito Mode**: Private conversations that aren't stored
- **Namespaces**: Context-specific querying (Course, Classroom)
- **Deep Search**: Enhanced search capabilities for complex queries
- **Feedback Collection**: Thumbs up/down on responses for quality tracking
- **Suggested Questions**: Guided starting points for new users

### Running the Frontend Locally

```bash
cd neu-chatbot
npm install
npm start
```

---

## Code Structure

- **`dags/scraping_pipeline_gcs.py`**:
  - **Tasks**:
    - `install_requirements`: Installs script dependencies.
    - `run_unittests`: Runs unit tests on the scraper.
    - `extract_data`: Executes the scraping script.
    - `validate_data`: Ensures scraped files exist.
    - `dvc_push`: Versions data with DVC and pushes to GCS.
    - `alert_failure`: Alerts on any task failure.
  - **Flow**:
    ```python
    (install_requirements >> run_unittests >> extract_data >> validate_data >> dvc_push)
    [run_unittests, extract_data, validate_data, dvc_push] >> alert_failure
    ```

- **`dags/gcs_to_pinecone_embedding.py`**:
  - Downloads files from GCS in batches, generates embeddings, and upserts to Pinecone.
  - Tasks: List files, download batches, process embeddings, upsert, cleanup.

- **`scripts/`**:
  - `Scrape_script.py`: Web scraping logic.
  - `requirements.txt`: Script dependencies.

- **`tests/`**:
  - `test_scraper.py`: Unit tests for the scraper.

- **`utils/`**:
  - Helper functions for configuration, storage, embeddings, and Pinecone.

---

## Reproducibility

To replicate the pipeline:

1. **Clone and Set Up**:
   - Follow [Setup Instructions](#setup-instructions).

2. **Pull Versioned Data with DVC**:
   - Exec into the Airflow container:
     ```bash
     docker exec -it <airflow-container-name> bash
     ```
   - Initialize DVC and pull data:
     ```bash
     dvc init
     dvc remote add -d my_gcs gs://your-gcs-bucket-name
     dvc pull
     ```

3. **Run the Pipeline**:
   - Enable DAGs as described in [Running the Pipeline](#running-the-pipeline).

- **Data Versioning**: DVC tracks `data/` and syncs with GCS via `dvc.yaml`.
- **Dependencies**: Managed via `scripts/requirements.txt`.

---

## Error Handling & Logging

- **Error Handling**:
  - Scraping failures raise `AirflowFailException`.
  - GCS and Pinecone tasks handle exceptions gracefully.
  - Batch processing adjusts dynamically to file counts.

- **Logging**:
  - Logs are stored in `logs/` and accessible via the Airflow UI.
  - Task-specific logs (e.g., file uploads) aid troubleshooting.

---

## Expected Outcome

- Once all tasks in `scraping_pipeline_gcs` complete successfully:
  - Scraped `.txt` files appear in your GCS bucket under the `scraped_texts/` prefix.
- After `gcs_to_pinecone_embedding` runs:
  - Embeddings are upserted to Pinecone, verifiable via the Pinecone dashboard.
 
## Workflow 
![image](https://github.com/user-attachments/assets/9953f038-09f0-48e7-9082-e17365088f13)

